#! /usr/bin/env python
from __future__ import print_function
import numpy as np
import os
#import textwrap
from scipy import interpolate
from scipy.interpolate import CloughTocher2DInterpolator
import astropy
from astropy import wcs
from astropy.io import fits
from astropy.io.votable import parse_single_table
from astropy.coordinates import SkyCoord, Angle, Latitude, Longitude, SkyOffsetFrame
from astropy.table import Table, hstack, join, vstack
import astropy.units as u
import sys
import glob
import argparse
#import psutil
import matplotlib.pyplot as plt
from matplotlib import gridspec
from copy import copy
import warnings
import logging
import logging.config
from functions.format_checks import *
from functions.matching import *

warnings.filterwarnings("ignore", category=UserWarning) 

data_dir=os.path.join("~",os.path.dirname(sys.executable),"maccas_catalog_formats/")

if __name__=='__main__':
        
        parser=argparse.ArgumentParser(prog='maccas',prefix_chars='-')
        
        necessary_inputs=parser.add_argument_group('Required inputs')
        necessary_inputs.add_argument('--target', dest='tar_cat', type=str, default=None, help='Target catalogue filename or path')
        necessary_inputs.add_argument('--reference', dest='ref_cat', type=str, default=None, help='Reference catalogue filename or path')
        necessary_inputs.add_argument('--freq', dest='tar_freq', type=float, default=None, help='Frequency of target catalogue on MHz')
        necessary_inputs.add_argument('--refsurvey', dest='ref_survey_name', type=str, default=None, help="Name of survey used as reference catalog, current support for 'GLEAM' and 'TGSS'. If using a different survey, Complete supplied template file 'reference_catalogue_format.txt' and call it here.")

        settings=parser.add_argument_group('Settings')
        settings.add_argument('--norestrict', dest='snr_init_restrict',action="store_false", default=True, help='Turn off initial restriction that matches only sources above a set SNR for the first run through')
        settings.add_argument('--overwrite', dest='output_overwrite', action="store_true", default=False,help='Overwrite files with same names as outputs, see documentation for these names')
        settings.add_argument('--nofluxmodel', dest='model_flux', action="store_false", default=True, help='Turn off large scale flux correction model. This is recommended only if target catalogue fluxes have been accurately calibrated')
        settings.add_argument('--warpplot', dest='plotting', action="store_true", default=False, help='Output the measured and modelled offsets from each iteration as and flux model (if turned on) as png files.')
        settings.add_argument('--nofluxmatch', dest='flux_match', action="store_false", default=True, help='Only assess match probability on physical proximity on sky. Note that this amounts to a nearest neighbour match with dewarping.')
        settings.add_argument('--debug', dest='debug',action="store_true", default=False, help="Turn on debugging [Default=False]")
        settings.add_argument('--writelog', dest='log_write', action='store_true', default=False, help="Write logged messages out to 'maccas_date_time.log' file [Default=False]")       
        
        optional_inputs=parser.add_argument_group('Optional inputs')
        optional_inputs.add_argument('--outformat', dest='output_format', type=str, default="fits", help="File format for table outputs, can be any format supported by astropy.table.Table object writing tool [Default=fits]")
        optional_inputs.add_argument('--singlepercentile', dest='single_match_percentile', type=float, default=0.95, help='Percentile above which single matches are accepted as true matches [Default=0.95]')
        optional_inputs.add_argument('--multipercentile', dest='multiple_match_percentile', type=float, default=0.6, help='Normalised percentile above which one of multiple  matches is accepted as true matches [Default=0.6]')
        optional_inputs.add_argument('--snr', dest='snr_restriction', type=float, default=10., help='SNR threshold required for sources to be matched in the first run [Default=10]')
        optional_inputs.add_argument('--modeldeg', dest='flux_model_deg', type=int, default=1, help='Degree of 2D polynomial used to model large scale flux correction (only integers between 1 and 5 inclusive) [Default=1]')
        optional_inputs.add_argument('--tarformat', dest='tar_table_formatting_file', type=str, default=None, help="File to translate column names from supplied target catalogue into useable form for algrotihm. Complete supplied template file 'target_catalogue_format.txt' and call it here [Default=None]")
        optional_inputs.add_argument('--filesuffix', dest='save_file_suffix', type=str, default=None, help="Suffix appended to all file names from impending run, provided as an alternative to renaming previous run files or overwriting them [Default=None]")
        
        
        options = parser.parse_args()

        #set up logging
        FORMAT = '%(asctime)s MACCAS %(levelname)s  %(message)s'
        logging.basicConfig(format=FORMAT,datefmt='%d/%m/%Y %H:%M:%S')
        log=logging.getLogger("MACCAS")
        logging_level = logging.DEBUG if options.debug else logging.INFO
        log.setLevel(logging_level)
        
        #set up file logging
        if options.log_write:
                import datetime
                t=datetime.datetime.now()
                suffix=str(t.day)+"-"+str(t.month)+"-"+str(t.year)+"_"+str(t.hour)+"-"+str(t.minute)+"-"+str(t.second)
                formatter=logging.Formatter(FORMAT,datefmt='%d/%m/%Y %H:%M:%S')
                fh=logging.FileHandler('maccas_log_'+suffix+'.log')
                fh.setLevel(logging.DEBUG)
                fh.setFormatter(formatter)
                log.addHandler(fh)
        

        #provides the user with help dialogue about the program
        if len(sys.argv) <= 1:
                parser.print_help()
                sys.exit(0)

        #checks the user has provided a target catalog. If not, raise a fatal error.
        if options.tar_cat!=None:
                if os.path.exists(options.tar_cat):
                        target_cat_filename=options.tar_cat
                        log.info("Target catalogue: {0}".format(target_cat_filename.split('/')[-1]))
                else:
                        log.error("{0} not found.".format(options.tar_cat))
                        sys.exit(1)
        else:
                log.error("No target catalogue supplied. Use argument '--target filename' to supply target catalog")
                sys.exit(1)

        #checks the user has provided a reference catalog. If not, raise a fatal error.
        if options.ref_cat!=None:
                if os.path.exists(options.ref_cat):
                        reference_cat_filename=options.ref_cat
                        log.info("Reference catalogue: {0}".format(reference_cat_filename.split('/')[-1]))
                else:
                        log.error("{0} not found.".format(options.ref_cat))
                        sys.exit(1)
        else:
                log.error("No reference catalogue supplied. Use argument '--reference filename' to supply target catalog")
                sys.exit(1)

        #checks the user has specified reference survey table format. If not, raise a fatal error
        if options.ref_survey_name!=None:
                if options.ref_survey_name.split('.')[-1]=='txt':
                        ref_format_filename=options.ref_survey_name
                        log.info("Reference catalogue format used: {0}".format(ref_format_filename.split('/')[-1]))
                else:
                        ref_format_filename=data_dir+options.ref_survey_name+'.txt'
                        log.info("Reference catalogue format used: {0}".format(options.ref_survey_name))
        else:
                log.error("No reference catalogue table format supplied. Please choose from 'GLEAM', 'TGSS' or specify file name for alternate table format.")
                sys.exit(1)

        #read in the reference survey file format
        ref_required_format=['RA_name','DEC_name','RA_error_name','DEC_error_name','PSF_semimajor_axis_name','PSF_semiminor_axis_name','source_semimajor_axis_name','source_semiminor_axis_name','source_position_angle_name','peak_flux_name','peak_flux_error_name','integrated_flux_name','local_rms_name','unique_identifier_name','frequency','frequency_prefix_or_suffix']
        with open(ref_format_filename) as f:
                inputs = [line.rstrip('\n').rstrip('\r') for line in f][:16]
        ref_values=[line.split('=')[1].strip(' ') for line in inputs]
        if [line.split('=')[0] for line in inputs]==ref_required_format:
                ref_ra, ref_dec, ref_err_ra, ref_err_dec, ref_psf_a, ref_psf_b, ref_a, ref_b, ref_pa,ref_peak_flux, ref_err_peak_flux, ref_int_flux,ref_local_rms, ref_uuid=ref_values[:14]
                if '-' in ref_values[14]:
                        ref_min_freq=ref_values[14].split('-')[0]
                        ref_max_freq=ref_values[14].split('-')[1]
                        ref_p_or_s=ref_values[15]
                        num_freq='multiple'
                else:
                        ref_freq=ref_values[14]
                        ref_p_or_s=''
                        num_freq='single'
        else:
                writing_format=[]
                for i in range(0,16):
                        writing_format.append(ref_required_format[i]+'=\n')
                with open('reference_catalogue_format.txt', 'w') as f: 
                        f.writelines(writing_format)
                log.error("{0} cannot be read. Please fill out 'reference_catalogue_format.txt' in your working directory and call it in --refsurvey".format(ref_format_filename))
                sys.exit(1)

        #checks if the user has specified a target catalogue table format. If not go to default
        if options.tar_table_formatting_file!=None:
                tar_format_filename=options.tar_table_formatting_file
                log.info("Target catalogue format used: {0}".format(tar_format_filename.split('/')[-1]))
        else:
                tar_format_filename=data_dir+'target_format_default.txt'
                log.info("Default target catalogue format used")

        #read in the target survey file format
        tar_required_format=['RA_name','DEC_name','RA_error_name','DEC_error_name','PSF_semimajor_axis_name','PSF_semiminor_axis_name','source_semimajor_axis_name','source_semiminor_axis_name','source_position_angle_name','peak_flux_name','peak_flux_error_name','integrated_flux_name','local_rms_name','unique_identifier_name']
        with open(tar_format_filename) as f:
                inputs = [line.rstrip('\n').rstrip('\r') for line in f][:14]
        tar_values=[line.split('=')[1].strip(' ') for line in inputs]
        if [line.split('=')[0] for line in inputs]==tar_required_format:
                tar_ra, tar_dec, tar_err_ra, tar_err_dec, tar_psf_a, tar_psf_b, tar_a, tar_b, tar_pa, tar_peak_flux, tar_err_peak_flux, tar_int_flux, tar_local_rms, tar_uuid=tar_values
        else:
                writing_format=[]
                for i in range(0,14):
                        writing_format.append(tar_required_format[i]+'=\n')
                with open('target_catalogue_format.txt', 'w') as f: 
                        f.writelines(writing_format)
                log.error("{0} cannot be read. Please fill out 'target_catalogue_format.txt' in your working directory and call it in --tarformat".format(tar_format_filename.split('/')[-1]))
                sys.exit(1)

        #apply all the run settings
        if options.snr_init_restrict:
                snr_restrict=options.snr_restriction
                log.info("Initial signal-to-noise ratio restriction: {0}".format(snr_restrict))
        else:
                snr_restrict=options.snr_init_restrict
                log.info("Initial signal-to-noise ratio restriction: Off")
        if options.output_overwrite:
                log.info("Overwrite exisiting output files: On")
        else:
                log.info("Overwrite exisiting output files: Off")
        if options.model_flux and options.flux_match:
                log.info("Flux modelling: On")
                log.info("2D flux model degree: {0}".format(options.flux_model_deg))
        else:
                log.info("Flux modelling: Off")
        if options.plotting:
                log.info("Plotting measured and modelled offsets: On")
        else:
                log.info("Plotting measured and modelled offsets: Off")
        if options.flux_match:
                log.info("Flux match: On")
                #checks the user has provided a frequency for the target catalogue. If flux matching is on and the target frequency is not the same as the reference frequency, then warnings an fatal errors shoudl be raised. If flux matching is off, this condition can be completely ignored. 
                if options.tar_freq!=None:
                        target_frequency=float(options.tar_freq)
                        log.info("Target catalogue frequency: {0} MHz".format(target_frequency))
                        if num_freq=='multiple':                        
                                if target_frequency<float(ref_max_freq) and target_frequency>float(ref_min_freq):
                                        log.info("Target catalogue frequency falls in reference catalog range, if no data exists in reference catalog for target catalog frequency exactly, then interpolation will be used.")
                                elif target_frequency>float(ref_max_freq) or target_frequency<float(ref_min_freq):
                                        log.warning("Target catalogue frequency falls outside reference catalog range. Extrapolation will be used, however results may be wildly inaccurate. Consider using a different catalogue that covers the target catalogue frequency or turning flux matching off, to match only on position (using --nofluxmatch).")
                                else:
                                        log.error("Unknown target frequency: {0}".format(target_frequency))
                                        sys.exit(1)
                        else:
                                if target_frequency!=float(ref_freq):
                                        log.warning("Target and reference catalogue frequency either do not match, or have not been specified. This algorthm will continue on the assumption that they do match or are close enough for comparison. If this is not the case, results may be wildly inaccurate. Consider using a different catalogue that covers the target catalogue frequency or turning flux matching off, to match only on position (using --nofluxmatch).") 
                else:
                        if num_freq=='multiple':
                                log.error("No target catalogue fequency supplied for comparison with a multiple frequency reference catalog. Please specify a target catalogue frequency using --freq")
                                sys.exit(1)
                        else:
                                log.warning("Target and reference catalogue frequency either do not match, or have not been specified. This algorthm will continue on the assumption that they do match or are close enough for comparison. If this is not the case, results may be wildly inaccurate. Consider using a different catalogue that covers the target catalogue frequency or turning flux matching off, to match only on position (using --nofluxmatch).")
        else:
                log.info("Flux match: Off")
                
        #We know exactly what is present in the GLEAM catalogue, so we will find the neighbouring frequencies 
        if options.ref_survey_name=='GLEAM':
                GLEAM_freq=['076', '084', '092', '099', '107', '115', '122', '130', '143', '151', '158', '166', '174', '181', '189', '197', '204', '212', '220', '227']
                if options.tar_freq in np.array(GLEAM_freq, dtype=float):
                        ref_freq=np.array(GLEAM_freq)[np.where(np.array(GLEAM_freq, dtype=float)==options.tar_freq)][0]
                        num_freq='single'
                else:
                        above_tar_freq=np.array(GLEAM_freq)[np.where(np.array(GLEAM_freq, dtype=float)>float(options.tar_freq))]
                        below_tar_freq=np.array(GLEAM_freq)[np.where(np.array(GLEAM_freq, dtype=float)<float(options.tar_freq))]
                        if len(above_tar_freq)==0:
                                ref_max_freq=GLEAM_freq[-1]
                                ref_min_freq=GLEAM_freq[-2]
                        elif len(below_tar_freq)==0:
                                ref_max_freq=GLEAM_freq[1]
                                ref_max_freq=GLEAM_freq[0]
                        else:
                                ref_max_freq=above_tar_freq[0]
                                ref_min_freq=below_tar_freq[-1]
        
        log.info("Output format: {0}".format(options.output_format))
        log.info("Single match confidence percentile: {0}".format(options.single_match_percentile))
        log.info("Multiple match confidence percentile: {0}".format(options.multiple_match_percentile))
        if options.save_file_suffix==None:
                options.save_file_suffix=''
                log.info("No file suffix supplied")
        else:
                options.save_file_suffix='_'+options.save_file_suffix
                log.info("File suffix: {0}".format(options.save_file_suffix))

        #we're going to quickly double check if the filenames already exist in our directory
        filenames=['leftover_reference_catalogue'+options.save_file_suffix+'.'+options.output_format,'leftover_target_catalogue'+options.save_file_suffix+'.'+options.output_format,'leftover_unwarped_target_catalogue'+options.save_file_suffix+'.'+options.output_format,'cross_matched_table'+options.save_file_suffix+'.'+options.output_format,"Modelled_offsets" +options.save_file_suffix+"_run_","Measured_offsets" +options.save_file_suffix+"_run_"]
        for i in range(0,4):
                if os.path.exists(filenames[i])==True:
                        if options.output_overwrite==False:
                                log.error("{0} already exists".format(filenames[i]))
                                sys.exit(1)
                        else:
                                log.warning("{0} will be overwritten".format(filenames[i]))
        if options.plotting==True:
                for i in range(4,6):
                        if len(glob.glob(filenames[i]+'*.png'))>0:
                                if options.output_overwrite==False:     
                                        log.error("Files beginning with {0} already exist".format(filenames[i]))
                                        sys.exit(1)
                                else:
                                        log.warning("Files beginning with {0} will be overwritten".format(filenames[i]))

        #bring the global variables in before running an external function
        if num_freq=='single':
                vars_g=[ options, None, None, ref_p_or_s, num_freq, ref_freq,log]
        else:
                vars_g=[ options, ref_min_freq, ref_max_freq, ref_p_or_s, num_freq,None,log]
        
        #We're now going to read in the catalogs, and check that the supplied column names exist in the table
        #Currently only fits support, but will aim to have support for other formats in future revisions
        raw_reference_table=np.squeeze(fits.open(reference_cat_filename)[1].data)
        raw_target_table=np.squeeze(fits.open(target_cat_filename)[1].data)
        
        target_table_length=len(raw_target_table)
        reference_table_length=len(raw_reference_table)
        
        #target catalog checking 
        for item in tar_values[:2]:
                try:
                        test=raw_target_table[item]
                except KeyError:
                        log.error("Could not find column '{0}' in '{1}'. Please edit reference catalogue format file to reflect target catalogue column names and call with '--tarformat filename'".format(item, target_cat_filename.split('/')[-1]))
                        sys.exit(1)
        tar_ra_data=raw_target_table[tar_values[0]]
        tar_dec_data=raw_target_table[tar_values[1]]
        tar_err_ra_data=check_column_exists(tar_values[2],raw_target_table,vars_g,optional=True,length=target_table_length,variable='target error RA',error=True)
        tar_err_dec_data=check_column_exists(tar_values[3],raw_target_table,vars_g,optional=True,length=target_table_length,variable='target error DEC',error=True)
        tar_psf_a_data=check_column_exists(tar_values[4],raw_target_table,vars_g,optional=True,length=target_table_length,variable='target psf a')
        tar_psf_b_data=check_column_exists(tar_values[5],raw_target_table,vars_g,optional=True,length=target_table_length,variable='target psf b')
        tar_a_data=check_column_exists(tar_values[6],raw_target_table,vars_g,optional=True,length=target_table_length,variable='target a')
        tar_b_data=check_column_exists(tar_values[7],raw_target_table,vars_g,optional=True,length=target_table_length,variable='target b')
        tar_pa_data=check_column_exists(tar_values[8],raw_target_table,vars_g,optional=True,length=target_table_length,variable='target pa')
        tar_peak_flux_data=check_column_exists(tar_values[9],raw_target_table,vars_g)
        tar_err_peak_flux_data=check_column_exists(tar_values[10],raw_target_table,vars_g,optional=True,length=target_table_length,variable='target error peak flux',error=True)
        tar_int_flux_data=check_column_exists(tar_values[11],raw_target_table,vars_g,optional=True,length=target_table_length,variable='target integrated/total flux')
        if snr_restrict!=False or options.model_flux==True:
                tar_local_rms_data=check_column_exists(tar_values[12],raw_target_table,vars_g)
        else:
                tar_local_rms_data=check_column_exists(tar_values[12],raw_target_table,vars_g,optional=True,length=target_table_length,variable='target local rms')
        tar_uuid_data=check_column_exists(tar_values[13],raw_target_table,vars_g)
        
        #reference catalog checking        
        for item in ref_values[:2]:
                try:
                        test=raw_reference_table[item]
                except KeyError:
                        log.error("Could not find column '{0}' in '{1}'. Please edit reference catalogue format file to reflect reference catalogue column names and call with '--refsurvey filename'".format(item, reference_cat_filename.split('/')[-1]))
                        sys.exit(1)
        ref_ra_data=raw_reference_table[ref_values[0]]
        ref_dec_data=raw_reference_table[ref_values[1]]
        ref_err_ra_data=check_column_exists(ref_values[2],raw_reference_table,vars_g,optional=True,length=reference_table_length,variable='reference error RA',error=True)
        ref_err_dec_data=check_column_exists(ref_values[3],raw_reference_table,vars_g,optional=True,length=reference_table_length,variable='reference error DEC',error=True)
        #at least one of the four columns from either catalogue needs to have data for the program to proceed, so we must force this last one to have data
        if np.sum([tar_err_ra_data,tar_err_dec_data,tar_psf_a_data,tar_psf_b_data])+np.sum([ref_err_ra_data,ref_err_dec_data])==0:
                ref_psf_a_data=check_column_exists(ref_values[4],raw_reference_table,vars_g)
                ref_psf_b_data=check_column_exists(ref_values[5],raw_reference_table,vars_g)
        else:
                ref_psf_a_data=check_column_exists(ref_values[4],raw_reference_table,vars_g,optional=True,length=reference_table_length,variable='reference psf a')
                ref_psf_b_data=check_column_exists(ref_values[5],raw_reference_table,vars_g,optional=True,length=reference_table_length,variable='reference psf b')
        ref_a_data=check_column_exists(ref_values[6],raw_reference_table,vars_g,optional=True,length=reference_table_length,variable='reference a')
        ref_b_data=check_column_exists(ref_values[7],raw_reference_table,vars_g,optional=True,length=reference_table_length,variable='reference b')
        ref_pa_data=check_column_exists(ref_values[8],raw_reference_table,vars_g,optional=True,length=reference_table_length,variable='reference pa')
        ref_peak_flux_data=check_column_exists(ref_values[9],raw_reference_table,vars_g)
        ref_err_peak_flux_data=check_column_exists(ref_values[10],raw_reference_table,vars_g,optional=True,length=reference_table_length,variable='reference error peak flux',error=True)
        ref_int_flux_data=check_column_exists(ref_values[11],raw_reference_table,vars_g,optional=True,length=reference_table_length,variable='reference integrated/total flux')
        #again, if the other columns have no data, this one must, so it cannot be optional
        if np.sum([tar_err_peak_flux_data,tar_local_rms_data])+np.sum([ref_err_peak_flux_data])==0:
                ref_local_rms_data=check_column_exists(ref_values[12],raw_reference_table,vars_g)
        else:
                ref_local_rms_data=check_column_exists(ref_values[12],raw_reference_table,vars_g,optional=True,length=reference_table_length,variable='reference local rms')
        ref_uuid_data=check_column_exists(ref_values[13],raw_reference_table,vars_g)
        
        #Finally we want to create Table objects for the reference and target catalogues for the other modules to easily use and reference
        raw_target_catalog=Table([tar_ra_data,tar_dec_data,tar_err_ra_data,tar_err_dec_data,tar_psf_a_data,tar_psf_b_data,tar_a_data,tar_b_data,tar_pa_data,tar_peak_flux_data,tar_err_peak_flux_data,tar_int_flux_data,tar_local_rms_data,tar_uuid_data],names=('ra','dec','err_ra','err_dec','psf_a','psf_b','a','b','pa','peak_flux','err_peak_flux','int_flux','local_rms','uuid'))
        raw_reference_catalog=Table([ref_ra_data,ref_dec_data,ref_err_ra_data,ref_err_dec_data,ref_psf_a_data,ref_psf_b_data,ref_a_data,ref_b_data,ref_pa_data,ref_peak_flux_data,ref_err_peak_flux_data,ref_int_flux_data,ref_local_rms_data,ref_uuid_data],names=('ra','dec','err_ra','err_dec','psf_a','psf_b','a','b','pa','peak_flux','err_peak_flux','int_flux','local_rms','uuid'))
        
        run(raw_target_catalog, raw_reference_catalog, snr_restrict,log,options)
